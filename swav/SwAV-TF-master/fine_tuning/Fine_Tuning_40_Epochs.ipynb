{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fine Tuning on Flower Dataset(40_epochs)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayulockin/SwAV-TF/blob/master/fine_tuning/Fine_Tuning_40_Epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IyLUFcoFPCIO"
      },
      "source": [
        "# Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v8fMXBGWztKi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "88f422fb-b397-4785-8758-82af6eb1637d"
      },
      "source": [
        "!git clone https://github.com/ayulockin/SwAV-TF.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SwAV-TF'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 207 (delta 5), reused 0 (delta 0), pack-reused 198\u001b[K\n",
            "Receiving objects: 100% (207/207), 15.83 MiB | 21.76 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1O-8mYyD0CiP",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('SwAV-TF/utils')\n",
        "\n",
        "import multicrop_dataset\n",
        "import architecture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R1c99jQL0KMr",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "from imutils import paths\n",
        "\n",
        "tf.random.set_seed(666)\n",
        "np.random.seed(666)\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4sEtMAFTPQI5"
      },
      "source": [
        "#### W&B - Experiment Tracking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iwpgB1O9PUO5",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W8aCpiQUPZCO",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7eaekTr10Yzn"
      },
      "source": [
        "## Restoring model weights from GCS Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LROY2Mrn0wpY",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import get_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZiEZfPfG0TeL",
        "colab": {}
      },
      "source": [
        "feature_backbone_urlpath = \"https://storage.googleapis.com/swav-tf/feature_backbone_40_epochs.h5\"\n",
        "prototype_urlpath = \"https://storage.googleapis.com/swav-tf/projection_prototype_40_epochs.h5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7pEVJ4Y70isN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "310b3033-e353-418b-e2a5-24530d42043b"
      },
      "source": [
        "feature_backbone_weights = get_file('swav_feature_weights', feature_backbone_urlpath)\n",
        "prototype_weights = get_file('swav_prototype_projection_weights', prototype_urlpath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/swav-tf/feature_backbone_40_epochs.h5\n",
            "94584832/94583160 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/swav-tf/projection_prototype_40_epochs.h5\n",
            "8839168/8831904 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vnhWEo8_12FK"
      },
      "source": [
        "## Dataset gathering and preparation\n",
        "\n",
        "We will be using only **10%** labeled training examples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BAOhOFMs110h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1f2dbc37-20bc-4c77-cea1-f0278e2e0f06"
      },
      "source": [
        "# Gather Flowers dataset\n",
        "train_ds, extra_train_ds, validation_ds = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:10%]\", \"train[10%:85%]\", \"train[85%:]\"],\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "@tf.function\n",
        "def scale_resize_image(image, label):\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, (224, 224)) # Resizing to highest resolution used while training swav\n",
        "    return (image, label)\n",
        "\n",
        "training_ds = (\n",
        "    train_ds\n",
        "    .map(scale_resize_image, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "testing_ds = (\n",
        "    validation_ds\n",
        "    .map(scale_resize_image, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset tf_flowers/3.0.0 (download: 218.21 MiB, generated: Unknown size, total: 218.21 MiB) to /root/tensorflow_datasets/tf_flowers/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset tf_flowers is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eqOBLHJa5Ztp"
      },
      "source": [
        "## Get SwAV architecture and Build Linear Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Znqofzfr46-4",
        "colab": {}
      },
      "source": [
        "def get_linear_classifier():\n",
        "    # input placeholder\n",
        "    inputs = Input(shape=(224, 224, 3))\n",
        "    # get swav baseline model architecture\n",
        "    feature_backbone = architecture.get_resnet_backbone()\n",
        "    # load trained weights\n",
        "    feature_backbone.load_weights(feature_backbone_weights)\n",
        "    feature_backbone.trainable = False\n",
        "\n",
        "    x = feature_backbone(inputs, training=False)\n",
        "    outputs = Dense(5, activation=\"softmax\")(x)\n",
        "    linear_model = Model(inputs, outputs)\n",
        "\n",
        "    return linear_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qtt4jmzzxe8Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "40601a2d-7bb4-4717-c297-3c161a5a27a7"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = get_linear_classifier()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 10245     \n",
            "=================================================================\n",
            "Total params: 23,597,957\n",
            "Trainable params: 10,245\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I0o5LHV0pCPu"
      },
      "source": [
        "## Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vECu62eyCdut",
        "colab": {}
      },
      "source": [
        "# Early Stopping to prevent overfitting\n",
        "early_stopper = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, verbose=2, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rp1fJIEYpFIC"
      },
      "source": [
        "# Without Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VEY1h0L2XjBJ"
      },
      "source": [
        "#### Warm Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RArUvFToDJKn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "outputId": "79bc5955-c4bb-4391-fc5a-1f8a58454169"
      },
      "source": [
        "# get model and compile\n",
        "tf.keras.backend.clear_session()\n",
        "model = get_linear_classifier()\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"],\n",
        "                     optimizer='adam')\n",
        "\n",
        "# initialize wandb run\n",
        "wandb.init(entity='authors', project='swav-tf')\n",
        "\n",
        "# train\n",
        "history = model.fit(training_ds,\n",
        "                 validation_data=(testing_ds),\n",
        "                 epochs=35,\n",
        "                 callbacks=[WandbCallback(),\n",
        "                            early_stopper])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 10245     \n",
            "=================================================================\n",
            "Total params: 23,597,957\n",
            "Trainable params: 10,245\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/authors/swav-tf\" target=\"_blank\">https://app.wandb.ai/authors/swav-tf</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/authors/swav-tf/runs/amz8akyn\" target=\"_blank\">https://app.wandb.ai/authors/swav-tf/runs/amz8akyn</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "6/6 [==============================] - 4s 710ms/step - loss: 2.7471 - acc: 0.2071 - val_loss: 1.9681 - val_acc: 0.2327\n",
            "Epoch 2/35\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 2.0741 - acc: 0.2834 - val_loss: 1.8378 - val_acc: 0.2073\n",
            "Epoch 3/35\n",
            "6/6 [==============================] - 4s 593ms/step - loss: 1.7793 - acc: 0.2752 - val_loss: 1.7979 - val_acc: 0.2273\n",
            "Epoch 4/35\n",
            "6/6 [==============================] - 4s 598ms/step - loss: 1.7349 - acc: 0.3188 - val_loss: 1.6523 - val_acc: 0.2600\n",
            "Epoch 5/35\n",
            "6/6 [==============================] - 3s 521ms/step - loss: 1.5726 - acc: 0.3188 - val_loss: 1.6656 - val_acc: 0.2218\n",
            "Epoch 6/35\n",
            "6/6 [==============================] - 3s 555ms/step - loss: 1.6101 - acc: 0.3351 - val_loss: 1.5940 - val_acc: 0.2836\n",
            "Epoch 7/35\n",
            "6/6 [==============================] - 3s 467ms/step - loss: 1.5127 - acc: 0.3297 - val_loss: 1.6146 - val_acc: 0.2455\n",
            "Epoch 8/35\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.5130 - acc: 0.3815Restoring model weights from the end of the best epoch.\n",
            "6/6 [==============================] - 3s 485ms/step - loss: 1.5130 - acc: 0.3815 - val_loss: 1.5944 - val_acc: 0.2364\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fk_M5nFZeEOr",
        "colab": {}
      },
      "source": [
        "model.save('warmup.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UIgLm-WzahA8"
      },
      "source": [
        "#### Fine tune CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6H3r1pXThx0S",
        "colab": {}
      },
      "source": [
        " def get_classifier():\n",
        "    # input placeholder\n",
        "    inputs = Input(shape=(224, 224, 3))\n",
        "    # get swav baseline model architecture\n",
        "    feature_backbone = architecture.get_resnet_backbone()\n",
        "    # load trained weights\n",
        "    feature_backbone.load_weights(feature_backbone_weights)\n",
        "    feature_backbone.trainable = True\n",
        "\n",
        "    # load warmup model\n",
        "    warmup_model = tf.keras.models.load_model('warmup.h5')\n",
        "    # get trained output layer\n",
        "    last_layer = warmup_model.get_layer('dense')\n",
        "\n",
        "    \n",
        "    x = feature_backbone(inputs, training=False)\n",
        "    outputs = last_layer(x)\n",
        "    linear_model = Model(inputs, outputs)\n",
        "\n",
        "    return linear_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6EHIchqPm_qj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "b25f4745-2eb8-49fe-af5e-3ecb8f37f73a"
      },
      "source": [
        "# get model and compile\n",
        "tf.keras.backend.clear_session()\n",
        "full_trainable_model = get_classifier()\n",
        "full_trainable_model.summary()\n",
        "\n",
        "full_trainable_model.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"],\n",
        "                     optimizer='adam')\n",
        "\n",
        "# initialize wandb\n",
        "wandb.init(entity='authors', project='swav-tf')\n",
        "\n",
        "# train                                                        \n",
        "history = full_trainable_model.fit(training_ds,\n",
        "                 validation_data=(testing_ds),\n",
        "                 epochs=35,\n",
        "                 callbacks=[WandbCallback(),\n",
        "                            early_stopper])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 10245     \n",
            "=================================================================\n",
            "Total params: 23,597,957\n",
            "Trainable params: 23,544,837\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/authors/swav-tf\" target=\"_blank\">https://app.wandb.ai/authors/swav-tf</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/authors/swav-tf/runs/2kw7cp8n\" target=\"_blank\">https://app.wandb.ai/authors/swav-tf/runs/2kw7cp8n</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "      2/Unknown - 1s 291ms/step - loss: 4.4187 - acc: 0.3516WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2179s vs `on_train_batch_end` time: 0.3634s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2179s vs `on_train_batch_end` time: 0.3634s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 7s 1s/step - loss: 3.8768 - acc: 0.2861 - val_loss: 2.0071 - val_acc: 0.3255\n",
            "Epoch 2/35\n",
            "6/6 [==============================] - 6s 1s/step - loss: 1.7826 - acc: 0.4305 - val_loss: 1.5661 - val_acc: 0.4564\n",
            "Epoch 3/35\n",
            "6/6 [==============================] - 6s 1s/step - loss: 1.3255 - acc: 0.5177 - val_loss: 1.3318 - val_acc: 0.5091\n",
            "Epoch 4/35\n",
            "6/6 [==============================] - 5s 842ms/step - loss: 0.8766 - acc: 0.6730 - val_loss: 1.3895 - val_acc: 0.5091\n",
            "Epoch 5/35\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.7302 - acc: 0.7166Restoring model weights from the end of the best epoch.\n",
            "6/6 [==============================] - 5s 859ms/step - loss: 0.7302 - acc: 0.7166 - val_loss: 1.5787 - val_acc: 0.4818\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_AZUPjx5Q6Wc"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dWmYfCyjQx6Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9f935e4e-308e-4485-cb7c-1cd3e5210aa5"
      },
      "source": [
        "loss, acc = full_trainable_model.evaluate(testing_ds)\n",
        "wandb.log({'Test Accuracy': round(acc*100, 2)})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 2s 175ms/step - loss: 1.3318 - acc: 0.5091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "966mDKFhqsrk"
      },
      "source": [
        "# Training with Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iPlQ3JKtq6j-"
      },
      "source": [
        "#### Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hACfOEyQnk4r",
        "colab": {}
      },
      "source": [
        "# Configs\n",
        "CROP_SIZE = 224\n",
        "MIN_SCALE = 0.5\n",
        "MAX_SCALE = 1.\n",
        "\n",
        "# Experimental options\n",
        "options = tf.data.Options()\n",
        "options.experimental_optimization.noop_elimination = True\n",
        "options.experimental_optimization.map_vectorization.enabled = True\n",
        "options.experimental_optimization.apply_default_optimizations = True\n",
        "options.experimental_deterministic = False\n",
        "options.experimental_threading.max_intra_op_parallelism = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hU2OZh1oTFaN",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def scale_image(image, label):\n",
        "\timage = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\treturn (image, label)\n",
        "\n",
        "@tf.function\n",
        "def random_apply(func, x, p):\n",
        "\treturn tf.cond(\n",
        "\t\ttf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "\t\t\t\ttf.cast(p, tf.float32)),\n",
        "\t\tlambda: func(x),\n",
        "\t\tlambda: x)\n",
        " \n",
        "@tf.function\n",
        "def random_resize_crop(image, label):\n",
        "  # Conditional resizing\n",
        "  image = tf.image.resize(image, (260, 260))\n",
        "  # Get the crop size for given min and max scale\n",
        "  size = tf.random.uniform(shape=(1,), minval=MIN_SCALE*260,\n",
        "\t\t          maxval=MAX_SCALE*260, dtype=tf.float32)\n",
        "  size = tf.cast(size, tf.int32)[0]\n",
        "  # Get the crop from the image\n",
        "  crop = tf.image.random_crop(image, (size,size,3))\n",
        "  crop_resize = tf.image.resize(crop, (CROP_SIZE, CROP_SIZE))\n",
        "  \n",
        "  return crop_resize, label\n",
        "\n",
        "@tf.function\n",
        "def tie_together(image, label):\n",
        "  # Scale the pixel values\n",
        "  image, label = scale_image(image , label)\n",
        "  # random horizontal flip\n",
        "  image = random_apply(tf.image.random_flip_left_right, image, p=0.5)\n",
        "  # Random resized crops\n",
        "  image, label = random_resize_crop(image, label)\n",
        "  \n",
        "  return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MLzlomx_d2iO",
        "colab": {}
      },
      "source": [
        "trainloader = (\n",
        "\ttrain_ds\n",
        "\t.shuffle(1024)\n",
        "\t.map(tie_together, num_parallel_calls=AUTO)\n",
        "\t.batch(BATCH_SIZE)\n",
        "\t.prefetch(AUTO)\n",
        ")\n",
        "\n",
        "trainloader = trainloader.with_options(options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PBzTIQAqtsLu"
      },
      "source": [
        "#### Warmup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pat13QzdtsL3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "87eecc49-d66a-442e-dd7b-3efa7ddbe4e4"
      },
      "source": [
        "# get model and compile\n",
        "tf.keras.backend.clear_session()\n",
        "model = get_linear_classifier()\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"],\n",
        "                     optimizer='adam')\n",
        "\n",
        "# initialize wandb run\n",
        "wandb.init(entity='authors', project='swav-tf')\n",
        "\n",
        "# train\n",
        "history = model.fit(trainloader,\n",
        "                 validation_data=(testing_ds),\n",
        "                 epochs=35,\n",
        "                 callbacks=[WandbCallback(),\n",
        "                            early_stopper])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 10245     \n",
            "=================================================================\n",
            "Total params: 23,597,957\n",
            "Trainable params: 10,245\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/authors/swav-tf\" target=\"_blank\">https://app.wandb.ai/authors/swav-tf</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/authors/swav-tf/runs/2vcpgws7\" target=\"_blank\">https://app.wandb.ai/authors/swav-tf/runs/2vcpgws7</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "6/6 [==============================] - 4s 654ms/step - loss: 2.4421 - acc: 0.2207 - val_loss: 2.1875 - val_acc: 0.2182\n",
            "Epoch 2/35\n",
            "6/6 [==============================] - 3s 535ms/step - loss: 1.9721 - acc: 0.1989 - val_loss: 1.8234 - val_acc: 0.2309\n",
            "Epoch 3/35\n",
            "6/6 [==============================] - 3s 574ms/step - loss: 1.9024 - acc: 0.2180 - val_loss: 1.7357 - val_acc: 0.2873\n",
            "Epoch 4/35\n",
            "6/6 [==============================] - 3s 537ms/step - loss: 1.6904 - acc: 0.2589 - val_loss: 1.7077 - val_acc: 0.2073\n",
            "Epoch 5/35\n",
            "6/6 [==============================] - 4s 592ms/step - loss: 1.5807 - acc: 0.3134 - val_loss: 1.6111 - val_acc: 0.2564\n",
            "Epoch 6/35\n",
            "6/6 [==============================] - 3s 482ms/step - loss: 1.5772 - acc: 0.3188 - val_loss: 1.6386 - val_acc: 0.2800\n",
            "Epoch 7/35\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.5726 - acc: 0.3079Restoring model weights from the end of the best epoch.\n",
            "6/6 [==============================] - 3s 478ms/step - loss: 1.5726 - acc: 0.3079 - val_loss: 1.6230 - val_acc: 0.2964\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hD-oqM7wslOr",
        "colab": {}
      },
      "source": [
        "model.save('warmup_augmentation.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q0IBzaXysj60"
      },
      "source": [
        "#### Fine tune CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OAn3v41qsj64",
        "colab": {}
      },
      "source": [
        " def get_classifier():\n",
        "    # input placeholder\n",
        "    inputs = Input(shape=(224, 224, 3))\n",
        "    # get swav baseline model architecture\n",
        "    feature_backbone = architecture.get_resnet_backbone()\n",
        "    # load trained weights\n",
        "    feature_backbone.load_weights(feature_backbone_weights)\n",
        "    feature_backbone.trainable = True\n",
        "\n",
        "    # load warmup model\n",
        "    warmup_model = tf.keras.models.load_model('warmup_augmentation.h5')\n",
        "    # get trained output layer\n",
        "    last_layer = warmup_model.get_layer('dense')\n",
        "\n",
        "    \n",
        "    x = feature_backbone(inputs, training=False)\n",
        "    outputs = last_layer(x)\n",
        "    linear_model = Model(inputs, outputs)\n",
        "\n",
        "    return linear_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3BoSzbemsj7F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "841dd065-d1c2-4df1-864e-441544bc3bc7"
      },
      "source": [
        "# get model and compile\n",
        "tf.keras.backend.clear_session()\n",
        "full_trainable_model = get_classifier()\n",
        "full_trainable_model.summary()\n",
        "\n",
        "full_trainable_model.compile(loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"],\n",
        "                     optimizer='adam')\n",
        "\n",
        "# initialize wandb\n",
        "wandb.init(entity='authors', project='swav-tf')\n",
        "\n",
        "# train                                                        \n",
        "history = full_trainable_model.fit(trainloader,\n",
        "                 validation_data=(testing_ds),\n",
        "                 epochs=35,\n",
        "                 callbacks=[WandbCallback(),\n",
        "                            early_stopper])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "functional_1 (Functional)    (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 10245     \n",
            "=================================================================\n",
            "Total params: 23,597,957\n",
            "Trainable params: 23,544,837\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/authors/swav-tf\" target=\"_blank\">https://app.wandb.ai/authors/swav-tf</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/authors/swav-tf/runs/3sacn0yp\" target=\"_blank\">https://app.wandb.ai/authors/swav-tf/runs/3sacn0yp</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "      2/Unknown - 1s 289ms/step - loss: 6.2898 - acc: 0.2344WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2147s vs `on_train_batch_end` time: 0.3630s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2147s vs `on_train_batch_end` time: 0.3630s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 7s 1s/step - loss: 4.5465 - acc: 0.2670 - val_loss: 2.7274 - val_acc: 0.2400\n",
            "Epoch 2/35\n",
            "6/6 [==============================] - 6s 1s/step - loss: 1.7255 - acc: 0.4060 - val_loss: 1.4001 - val_acc: 0.4745\n",
            "Epoch 3/35\n",
            "6/6 [==============================] - 6s 1s/step - loss: 1.3260 - acc: 0.5286 - val_loss: 1.3200 - val_acc: 0.4855\n",
            "Epoch 4/35\n",
            "6/6 [==============================] - 5s 840ms/step - loss: 1.0877 - acc: 0.6049 - val_loss: 1.3471 - val_acc: 0.4764\n",
            "Epoch 5/35\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.2733 - acc: 0.5286Restoring model weights from the end of the best epoch.\n",
            "6/6 [==============================] - 5s 862ms/step - loss: 1.2733 - acc: 0.5286 - val_loss: 1.3212 - val_acc: 0.4764\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FaKpRTQVXkF9"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zQAM4wjBXkGK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2fa2cb60-f24d-4ad6-e556-2cb75d7c7008"
      },
      "source": [
        "loss, acc = full_trainable_model.evaluate(testing_ds)\n",
        "wandb.log({'Test Accuracy': round(acc*100, 2)})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 2s 180ms/step - loss: 1.3200 - acc: 0.4855\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}